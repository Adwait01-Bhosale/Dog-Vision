# -*- coding: utf-8 -*-
"""DogBreed.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18PBvO2nO42cKrPUdCeBIloCLhKg2liYm

# End-to-end Multi class Dog Breed Classification

This notebook builds an Multi class image classifer using Tesorflow 2.0 and Tenserflow Hub.

## 1.Problem

Identify the Breed of the dog

When i'm sitting at a cafe and i take a photo of a dog,I want to know what breed of dog it is.

## 2.Data

## 3. Evaluation

## 4.Features

Some information about the data:

* We're dealiing with images (unstructured data) so it's probably best we use deep learning/transfer learning.

* There are 120 breeds of dogs (this means there are 120 different classes)

* There are around 10,000+ images in the traing set (these images have label)

* There are around 10,000+ images in the test set(these umages have no labels,because we'll want to predict them.)
"""

# Unzipping the data.
# !unzip "drive/MyDrive/Dog Vision/dog-breed-identification.zip" -d "drive/MyDrive/Dog Vision/"

"""## Getting our workspace ready

* Importing Tesorflow
* Import Tesorflow Hub.
* Make sure we're using a GPU


"""

# Import Necessary Tools.

import tensorflow_hub as hub
import tensorflow as tf
print("Tf version:",tf.__version__)
print("hub version:",hub.__version__)

# Check for GPU availability 

print("GPU","available (YESSSSS!!!!)" if tf.config.list_physical_devices("GPU") else "not available:(")

"""## Getting our data (turning into tensors)

With all machine learning models, our data has to be in numeric format.So othat's what we'll be doing first .Turning our images into Tensors.

Let's start by accessing our data and checking out the data
"""

# Check the labels of our data
import pandas as pd
labels_csv=pd.read_csv("drive/MyDrive/Dog Vision/labels.csv")
print(labels_csv.describe())
print(labels_csv.head())

labels_csv.head()

# How many images are there of each breed?
labels_csv['breed'].value_counts()

# Plotting the breed

labels_csv['breed'].value_counts().plot.bar(figsize=(20,10))

labels_csv['breed'].value_counts().median()

# Let's View an image
from IPython.display import Image
Image("drive/MyDrive/Dog Vision/train/001513dfcb2ffafc82cccf4d8bbaba97.jpg")

"""## Getting Images and labels 

Let's get a list of all our image file pathnames
"""

labels_csv.head()

filenames=["drive/MyDrive/Dog Vision/train/"+ fname + ".jpg" for fname in labels_csv["id"]]
filenames[:10]

# Check whether number of filenames matches number of actual image files
import os 
# os.listdir("drive/MyDrive/Dog Vision/train/")[:10]
if len(os.listdir("drive/MyDrive/Dog Vision/train/"))==len(filenames):
  print("Filenames match actual actual amount of files!")
else:
  print("Filenames do not match!")

# One more check
Image(filenames[9000])

# Let's Check Out the name of the breed
labels_csv['breed'][9000]

"""Since we've now got our trainig image filepaths in a list let's prepare our labels."""

import numpy as np
labels=labels_csv['breed'].to_numpy()
# labels=np.array(labels)
labels

len(labels)

# see if number of labels matches the number of filenames
if len(labels)==len(filenames):
  print("Matched Successfully!!!")

else:
  print("Not Matched!!")

"""# No. of lebels matches the filenames

## Finding the Unique labels
"""

unique_breed=np.unique(labels)
unique_breed

len(unique_breed)

print(labels[0])
labels[0]==unique_breed

"""# Turn every label into a boolean array"""

boolean_labels=[label==unique_breed for label in  labels]

boolean_labels[:2]

len(boolean_labels)

# Example Turning boolean array into integers

print(labels[0]) # original label
print(np.where(unique_breed==labels[0])) # index where labels occur
print(boolean_labels[0].argmax()) # index where label occurs in boolean array
print(boolean_labels[0].astype(int)) # there will be a 1 where the sample labels occurs

"""### Creating our own validation set

Since the dataset from kaggle come with a validation set,we're going to create our own.
"""

# Setup X and y variables
X=filenames
y=boolean_labels

"""We're going to start off experimenting with ~1000 images and increase as needed"""

# Set number of images use for experimenting
NUM_IMAGES=1000 #@param{type:"slider",min:1000,max:10000,step:1000}

# Let's split our data into train and validation
from sklearn.model_selection import train_test_split

# Spliting them into training and validation of total size NUM_IMAGES

X_train,X_val,y_train,y_val=train_test_split(X[:NUM_IMAGES],
                                               y[:NUM_IMAGES],
                                               test_size=0.2,
                                               random_state=42)
len(X_train),len(y_train),len(X_val),len(y_val)

# Let's have a geez at the training data
X_train[:5],y_train[:2]

"""## Preprocessing images(turning images into Tensors)
To process our iamges into Tensors we're going to write a function which does a few things

1. Take an image filepath as input

2. Use Tensorflowe to read the file and save it to a variable, `image`

3. Turn our `image` (a jpg) into Tensors

4. Resize the image to be a shape(224,224)

5. Return the modified image

Before we do,let's see what importing an image looks like.
"""

# Conver an image into a numpy array

from matplotlib.pyplot import imread
image=imread(filenames[42])
image.shape

image

image.max() , image.min()

tf.constant(image)[:2]

"""Now we've seen what an image looks like as a Tensor,let's make a function to prepare them

1. Take an image filepath as input

2. Use Tensorflowe to read the file and save it to a variable, `image`

3. Turn our `image` (a jpg) into Tensors

4. Normallize our image(convert color channel value from 0-255 to 0-1)

5. Resize the image to be a shape(224,224)

6. Return the modified image
"""

# Define image size
IMG_SIZE=224

# Create a function for preprocessing images
def process_image(image_path,img_size=IMG_SIZE):
  """
  Takes an image filepath and turn the image into a Tensor.
  
  """

  # Read the image file
  image=tf.io.read_file(image_path)
 
  # Turn the jpeg into a numeric Tensor with 3 colour channels(RED,GREEN,BLUE)
  image=tf.image.decode_jpeg(image,channels=3)
  # convert the colour channels values from 0-255 to 0-1 values
  image=tf.image.convert_image_dtype(image,tf.float32)
  # Resize the image(224,224)
  image=tf.image.resize(image,size=[IMG_SIZE,IMG_SIZE])
  return image

tensor=tf.io.read_file(filenames[26])
tensor

tensor=tf.image.decode_jpeg(tensor,channels=3)

tf.image.convert_image_dtype(tensor,tf.float32)

"""## Turning data into batches

Why turn our data into batches?


Let's say we're trying to process 10000+ images in one  go... all might not fit into memeory.

So that  why we do about 32 (this is the batch size) images a time (we can manually adjust the batch size if need be).

In order to use Tensorflow effectively,we need our data 9 the form of Tensors tuples ehicj look like this:

`(image,labels)`
"""

# Create a simple function to retun a tupple of tenses

def get_image_label(image_path,label):
  """
  Takes an image file path name and the associated label,
  processes the image and returns a type of(image,label).
  """

  image=process_image(image_path)
  return image,label

(process_image(X[42]),tf.constant(y[42]))

"""Now we've got a way to turn our data into tuples of Tensors in the form (`image ,labels`).let's make a function to turn all our data (x and y) into batches"""

# Define the batch size,32 is a good start

BATCH_SIZE=32
def create_data_batches(X,y=None,batch_size=BATCH_SIZE,valid_data=False,test_data=False):
  """
  Creates batches of data out of image (X) and label(y) pairs.
  Shuffles the data if it's training data but doesn't shuffle if it's validation data.
  Also accepts test data as input(no labels).
  """

  # If the data is a test dataset,we probably don't have labels.
  if test_data:
    print("Creating test_data batches")
    data=tf.data.Dataset.from_tensor_slices((tf.constant(X))) # only filepaths (nolabels)
    data_batch=data.map(process_image).batch(BATCH_SIZE)
    return data_batch
  # If the data is a valid dataset,we don't need to shuffle it
  elif valid_data:
    print("Creating validation data batches...")
    data=tf.data.Dataset.from_tensor_slices((tf.constant(X), # filepaths
                                             tf.constant(y))) #labels

    data_batch=data.map(get_image_label).batch(BATCH_SIZE)
    return data_batch

  else:
    print("Creating training data batches...")
    data=tf.data.Dataset.from_tensor_slices((tf.constant(X),
                                             tf.constant(y)))
    
    # Shuffling pathnames and labels before mapping image processor function is faster than shuffling images

    data=data.shuffle(buffer_size=len(X))

    # Create (image,label) tuples (this also convert the image path into a processed image)
    data=data.map(get_image_label)

    # Turning the training data into batches
    data_batch=data.batch(BATCH_SIZE)
  return data_batch

# Create training and validation data batches
train_data=create_data_batches(X_train,y_train)
val_data=create_data_batches(X_val,y_val,valid_data=True)

# Check out the different attributes of our data batches
train_data.element_spec,val_data.element_spec

"""## Visualizing Data Batches

Our data is now in batches,however these canz be a little hard to understand/comprehand,let's visualize them!


"""

import matplotlib.pyplot as plt

# Create a function for viewing images in a data batches
def show_25_images(images,labels):
  """
  Display a plot of 25 images and their labels from a data batch.
  
  """

  # Setup the figure
  plt.figure(figsize=(10,10))
  # Lopp through 25(for displaying 25 images)
  for i in range(25):
    # Create subplots(5 rows and 5 cols)
    ax=plt.subplot(5,5,i+1)
    # Display an image
    plt.imshow(images[i])
    # Add the image label as the title
    plt.title(unique_breed[labels[i].argmax()])
    # Turn the grid lines off
    plt.axis("off")

"""## As the entirte data is converted into batches, to visualize it we need to unbatch it."""

train_data

train_images,train_labels=next(train_data.as_numpy_iterator())
train_images,train_labels

unique_breed[y[0].argmax()]

unique_breed

"""### The above unbatching code will first return the training images in the unbatched form and then the training_labels 

* The term (next) just grabs the first bach and just goes on moving to the next labels
"""

len(train_images),len(train_labels)

show_25_images(train_images,train_labels)

"""## Buid a model 
Before we buid a model, there are a few things we need to define:
* The input shape(ourimages shape, in form of Tensors) to our model.
* The output shape(image labels,in the form of Tensors) of ouor model.
* The URL of the model we want to use(From TensorFlow Hub).

"""

IMG_SIZE

len(unique_breed)

# Setup input shape to the model
INPUT_SHAPE=[None,IMG_SIZE,IMG_SIZE,3] # batch,height,width,colour channels

# Setup output shape of our model
OUTPUT_SHAPE=len(unique_breed)

# Setup model URL from TensorFlow Hub
MODEL_URL="https://tfhub.dev/google/imagenet/mobilenet_v2_130_224/classification/4"

"""### Now we've got our input's,outputs and model ready to go.Let's put them together into a Keras Deep Learning model!

* Knowing this,let's create a function which:
     * Takes the input shape,output shape and the model we'e choosen as a parameter.
     * Defines the layer in a Keras model in Sequential fashion(do this first ,then this, then that)
     * Compiles the model cells how it should be evaluates and improved.
     * Builds the model tells the model(tells the ,model the input shape it'll be getting).
     * Return the Model

* Note:
  * In case of Binary Classificatin we use activation as `Sigmoid` and in clase of Multi class classification we use `Softmax`
  * This activation converts the patterns into `0` and `1`
  * And the Highest Value becomes Our label
  * Generally Mobilenetv2 Converts our model into 1128 patterns but below we've given it as len(unique_brees) which take only 120 units.
"""

# Create a Function Which Builds a Keras Model


def create_model(input_shape=INPUT_SHAPE,output_shape=OUTPUT_SHAPE,model_url=MODEL_URL):
  print("Building Model with:",MODEL_URL)
  # Setup the model Layers
  model=tf.keras.Sequential([
  hub.KerasLayer(MODEL_URL), # Layer 1 (input_layer)
  tf.keras.layers.Dense(units=OUTPUT_SHAPE,
                        activation="softmax") # Layer 2 (output layer)
                        ])
  # Compile the model
  model.compile(loss=tf.keras.losses.CategoricalCrossentropy(),
                optimizer=tf.keras.optimizers.Adam(),
                metrics=["accuracy"])
  # Build the model
  model.build(INPUT_SHAPE)
  return model

model=create_model()
model.summary()

outputs=np.ones(shape=(1,1,1280))
outputs

"""### Create Some callbacks 
callbackd are helper functions a model can use during training to do such things as its progress,check its progress or stop training early if a model stops improving

We'll Create two Call backs,one for TensorBoard which helps track our model progress and another for each stopping which prevents pour model from training for too long.

### TensorBoard callback:

=> This helps Monitoring our model performance as its training, figuring out if it's learning correctly is it doing well or not.
     
To setup a TensorBoard callback, we need to do 3 things:
1. Load the TensorBoard notebook extension.
2. Create a TensorBoard callback which is able to save logs.
3. Visualize our models training logs with `%tensorboar` magic function (we'll do this after model training)
"""

# Commented out IPython magic to ensure Python compatibility.
# Load TensotrBoard notebook extension
# %load_ext tensorboard

import datetime
# Create a function to build a TensorBoard callback
def create_tensorboard_callback():
  # Create a log Dictionary for TensorBoard logs
  logdir=os.path.join("drive/MyDrive/Dog Vision/logs",
                      # Make it so the experiments get tracked whenever we run an experiment
                      datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))
  return tf.keras.callbacks.TensorBoard(logdir)

"""### Early Stopping Callback

Early stopping helps stop our model from overfitting by stopping training if a certain evaluation metric stops improving.

https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping

"""

# Create early stopping callback

early_stopping=tf.keras.callbacks.EarlyStopping(monitor="val_accuracy",
                                                patience=3)

"""## Training a model (on subset of data)

our first model is only going to traing on 1000 images,to make sure everything is working
"""

NUM_EPOCHS=100 #@param {type:"slider",min:10,max:100,step:10}

# Check to make sure we're still running on a GPU
print("GPU availble YESS!!!!!"if tf.config.list_physical_devices("GPU") else "Not available")

"""# Let's Create a function which trains a model.
* Create a model using `create_model()`
* Setup a TensorBoard callback `create_tensorboard_callback()`
* Call the function on our model passing it in training data,validation data ,number of epochs to train for(`NUM_EPOCHS`) andf the callbacks we'd like to used
* Return the model  
"""

# Build a function to train and return a trained model

def train_model():
  # Create a model
  model= create_model()

  # Create new TensorBoard session everytime we train a model
  tensorboard=create_tensorboard_callback()

  # Fit the model to the data passing it the callbacks we created
  model.fit(x=train_data,
            epochs=NUM_EPOCHS,
            validation_data=val_data,
            validation_freq=1,
            callbacks=[tensorboard,early_stopping])
  # Return yhe fitted model
  return model

# Fit the model to the data

model=train_model()
model

"""* It looks like our model is overfitting because it's overfitting far better on the training dataset than the validation dataset,what are some ways to prevent model overfitting in deep learning neural networks

### But But But Overfitting is a good thing at the begining which means it's learning

### Checking the TensorBoard logs

The TensorBoard Magic function(`%tensorboard`) will access the logs directory we created earlier and visualize it's contents
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir drive/MyDrive/Dog\ Vision/logs

"""## making and evaluating predictions using a trained model"""

# Make predictions on validation data (not used to train on)

predictions=model.predict(val_data,verbose=1)
predictions

predictions.shape

np.sum(predictions[0])

np.sum(predictions[1])

# First Prediction
index=42
print(predictions[index])
print(f"Max value (Probability of prediction){np.max(predictions[index])}")
print(f"Sum: {np.sum(predictions[index])}")
print(f"Max index: {np.argmax(predictions[index])}")
print(f"Predicted label:{unique_breed[np.argmax(predictions[index])]}")

unique_breed[113]

"""#### Having the above functionality is great but we want to do it at scale.
#### And it would be even better if we could see the image the predictions is being made on!
### Note:
      * Prediction probabilities are also known as confidence levels
"""

# Turn prediction probabilities into their Respective label(easier to understand)
def get_pred_label(prediction_probabilities):
  """"
  Turns  an array of prediction probabilities into a label.
  """
  return unique_breed[np.argmax(prediction_probabilities)]

# Get a predicted label based on an array of prediction probabilities.
pred_label=get_pred_label(predictions[81])
pred_label

"""Now since oour dataset is still in batch dataset ,we'll have to unbatchify it to make predictions on validation iamges and then compare those predictions to the validation labels(truth labels)."""

def unbatchify(data):
  images=[]
  labels=[]

  # Lopp through unbatched data
  for image,label in data.unbatch().as_numpy_iterator():
    images.append(image)
    labels.append(unique_breed[np.argmax(label)])
  return images,labels
# Unbatchify the validation data
val_images,val_labels=unbatchify(val_data)
val_images[0],val_labels[0]

get_pred_label(val_labels[0])

"""Now we've got ways to get :
* Prediction labels
* validation label(truth labels)
* Validation images
Let's make some functions to make these all a bit more visualize.

We'll Create a function which:
* Takes an array of ptediction probabilities, an array of truth labels and an array of images and integers.
* Convert the prediction probabilities to a predicte label.
* Plot the predicted label, its predicted probability, the truth label and the target image on a single plot.
"""

def plot_pred(prediction_probabilities,labels,images,n=1):
  """
  View the prediction , ground truth and image for sample n
  """
  pred_prob,true_label,image=prediction_probabilities[n],labels[n],images[n]

  # Get the pred_label
  pred_label=get_pred_label(pred_prob)

  # Plot image and remove ticks
  plt.imshow(image)
  plt.xticks([])
  plt.yticks([])

  # Change the colour of the title depending on if the prediction is right or wrong
  if pred_label==true_label:
    color='green'
  else:
    color='red'

  # Change plot title to be predicted,probability of prediction and truth label
  plt.title ("{} {:2.0f}% {}".format(pred_label,
                                     np.max(pred_prob)*100,
                                     true_label),
                                     color=color)

plot_pred(prediction_probabilities=predictions,
          labels=val_labels,
          images=val_images,
          n=77)

"""Now we've got one function to visualize our models top predictions,let's make another to view our models top 10 predictions 

This Functions will:
  * Take an input of predictions array and a ground array and an integer
  * Prediction probabilities values
  * Prediction labels

* Plot the top 10 predictio probability values and labels,coloring the true label green.
"""

def plot_pred_conf(prediction_probabilities,labels,n=1):
  """
  Plus the top 10 highest prediction confidence along with the truth labels for sample n.
  
  """
  pred_prob,true_label=prediction_probabilities[n],labels[n]

  # Get the predicted label
  pred_label=get_pred_label(pred_prob)

  # Find the top 10 prediction confidence indexes 
  top_10_pred_indexes=pred_prob.argsort()[-10:][::-1]

  # Find the top 10 prediction confidence values
  top_10_pred_values=pred_prob[top_10_pred_indexes]
  # Find the top 10 prediction labels
  top_10_pred_labels=unique_breed[top_10_pred_indexes]

  # Setup plot
  top_plot=plt.bar(np.arange(len(top_10_pred_labels)),
                   top_10_pred_values,
                   color='grey')
  plt.xticks(np.arange(len(top_10_pred_labels)),
             labels=top_10_pred_labels,
             rotation='vertical')
  # Change color of true label
  if np.isin(true_label,top_10_pred_labels):
    top_plot[np.argmax(top_10_pred_labels==true_label)].set_color('green')
  else:
    pass

plot_pred_conf(prediction_probabilities=predictions,
               labels=val_labels,
               n=9)

"""#### Now we've got some functions to help us visualize our predictions and evaluate our model ,let's check out a few"""

# Let's check out a few predictions and their different values

i_multiplier=20
num_rows=3
num_cols=2
num_images=num_rows*num_cols
plt.figure(figsize=(10*num_cols,5*num_rows))
for i in range(num_images):
  plt.subplot(num_rows,2*num_cols,2*i+1)
  plot_pred(prediction_probabilities=predictions,
            labels=val_labels,
            images=val_images,
            n=i+i_multiplier)
  plt.subplot(num_rows,2*num_cols,2*i+2)
  plot_pred_conf(prediction_probabilities=predictions,
                 labels=val_labels,
                 n=i+i_multiplier)
plt.tight_layout(h_pad=1.0)
plt.show()

"""## Saving and Reloading the model"""

# Create a function to save and load  a model
def save_model(model,suffix=None):
  """
  Saves a given model in a modeldirectory and appends a suffix (string)
  """

  # create a model directory pathname with current time
  modeldir=os.path.join("drive/MyDrive/Dog Vision/models",
                        datetime.datetime.now().strftime("%Y%m%d-%H%M%s"))
  model_path=modeldir + "-" + suffix + ".h5" # save format of model
  print(f"Saving model to: {model_path}...")
  model.save(model_path)
  return model_path

# Create a function to load a trained model

def load_model(model_path):
  """
  Loads a saved model from a specified from a specified path.
  """
  print(f"Loading saved model from {model_path}")
  model=tf.keras.models.load_model(model_path,
                                   custom_objects={"KerasLayer":hub.KerasLayer})
  return model

"""Now we've got functions to load a trained model,let's amke sure they work!!"""

# Save our model trained on 1000 images

save_model(model,suffix="1000-images-mobilenetv2-Adam")

# Evaluate Pre saved model
model.evaluate(val_data)

"""## Trainig a big dog model (on the full data)"""

len(X),len(y)

# Create a data batch with the ful data set

full_data=create_data_batches(X,y)

full_data

# Create a model for full model
full_model=create_model()

# Create full model callbacks
full_model_tensorboard=create_tensorboard_callback()
# No validation set when training on all the data, so we can't monitor validation accuracy

full_model_early_stopping=tf.keras.callbacks.EarlyStopping(monitor='accuracy',
                                                           patience=3)

"""**Note** :Running the cell below will take a little while(may upto 30 minutes for the first epoch) because the GPU we're using in the runtime has to load all of the images into memory"""

full_model.fit(x=full_data,
               epochs=NUM_EPOCHS,
               callbacks=[full_model_tensorboard,full_model_early_stopping])

save_model(full_model,suffix="full-image-set-mobilenetv2-Adam")

loaded_full_model=load_model('drive/MyDrive/Dog Vision/models/20210627-15261624807582-full-image-set-mobilenetv2-Adam.h5')

"""## Making Predictions on a test dataset

Since our model has been trained on images in the form of Tensors batches, to make predictions on the test data, we'll have to get it into the same format

Luckily we created `create_data_batches()` earluier which take a list of filenames as input and convert them into Tensor batches

To make predictions on the test data, we'll:

* Get the test image filenames
* Convert the filenames into test data batches using `craete_data_batches()` and setting the `test_data` parameter to `True` (Since the test data doesn't have labels).
* Make a prediction array by passing the test data batches to the predict() method called on our model.
"""

# Load test image filenames
test_path="drive/MyDrive/Dog Vision/test/"
test_filenames=[test_path + fname for fname in os.listdir(test_path)]
test_filenames[:10]

test_data = create_data_batches(test_filenames,test_data=True)

test_data

"""**Note** Calling predict on our full model and passing it the test data batch will take a long time to run(1 Hr)"""

# Make predictions on test data batches using the loaded full model

test_predictions=loaded_full_model.predict(test_data,
                                           verbose=1)

# Save predictions (Numpy array) to csv file (for access later)
np.savetxt("drive/MyDrive/Dog Vision/preds_array.csv",test_predictions,delimiter=",")

test_predictions=np.loadtxt("drive/MyDrive/Dog Vision/preds_array.csv",delimiter=",")

test_predictions.shape

test_predictions[:10]

"""## Making predictions on custom images

To make predictions on custom images,we'll:
* Get the file paths of our own images.
* Turn the filepaths into data batches using `create_data_batches()`.And since our custom images won't have labels, we set the `test data` parameter to `True`
* Pass the custom iamge data batch to our model's `predict()` method.
* Convert the predictions output probabilities to prediction labels
* Compare the predicted labels to custom images.
 
"""

# Get custom_image filepaths
custom_path="drive/MyDrive/Dog Vision/my-dog-photos/"
custom_image_paths=[custom_path + fname for fname in os.listdir(custom_path)]

custom_image_paths

# Turn custom images into batch datasets

custom_data=create_data_batches(custom_image_paths,test_data=True)
custom_data

# Make Predictions on the custom data
custom_preds=loaded_full_model.predict(custom_data)

custom_preds.shape

# Get Custom image prediction labels
custom_pred_labels=[get_pred_label(custom_preds[i]) for i in range(len(custom_preds))]
custom_pred_labels

# Get custom images (our unbatchify() function won't work since there aren't labels..)
custom_images=[]

# Loop through unbatch data
for image in custom_data.unbatch().as_numpy_iterator():
  custom_images.append(image)

# Check custom image predictions 
plt.figure(figsize=(10,10))
for i, image in enumerate(custom_images):
  plt.subplot(1,3,i+1)
  plt.xticks([])
  plt.yticks([])
  plt.title(custom_pred_labels[i])
  plt.imshow(image)

